{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data by removing unnecesary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data before cleaning: (39644, 61)\n",
      "Shape of data after cleaning: (39644, 54)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"OnlineNewsPopularity.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# print(\"Dataset Info:\")\n",
    "# print(df.info())\n",
    "\n",
    "# print(\"\\nFirst few rows:\")\n",
    "# print(df.head())\n",
    "\n",
    "# Remove the spaces at the start of each column name\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "# Drop unwanted columns\n",
    "columns_to_remove = [\"url\", \"timedelta\", \"LDA_00\", \"LDA_01\", \"LDA_02\", \"LDA_03\", \"LDA_04\"]\n",
    "df_cleaned = df.drop(columns=columns_to_remove)\n",
    "\n",
    "# print(\"\\nColumns after removal:\")\n",
    "# print(df_cleaned.columns)\n",
    "\n",
    "# Check for missing values\n",
    "# missing_values = df_cleaned.isnull().sum()\n",
    "# print(\"\\nMissing Values:\")\n",
    "# print(missing_values[missing_values > 0])  # Only display columns with missing values\n",
    "\n",
    "# Summary statistics\n",
    "# print(\"\\nSummary Statistics:\")\n",
    "# print(df_cleaned.describe())\n",
    "\n",
    "print(f\"Shape of data before cleaning: {df.shape}\")\n",
    "print(f\"Shape of data after cleaning: {df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a \"popularity\" column, based on the number of shares an article has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity\n",
      "medium    14265\n",
      "high      12955\n",
      "low       12424\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After adding this new column called 'popularity', the shape of the data is: \n",
      "(39644, 55)\n",
      "\n",
      "An article is considered low popularity if it less than 1000.0 shares\n",
      "An article is considered medium popularity if it has 1000.0 - 2100.0 shares\n",
      "An article is considered high popularity if it has more than 2100.0 shares\n"
     ]
    }
   ],
   "source": [
    "low_threshold = df_cleaned[\"shares\"].quantile(0.31)\n",
    "high_threshold = df_cleaned[\"shares\"].quantile(0.67)\n",
    "\n",
    "# Create the popularity category\n",
    "def classify_popularity(shares):\n",
    "    if shares <= low_threshold:\n",
    "        return \"low\"\n",
    "    elif shares <= high_threshold:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "df_cleaned[\"popularity\"] = df_cleaned[\"shares\"].apply(classify_popularity)\n",
    "\n",
    "# Verify the distribution\n",
    "print(df_cleaned[\"popularity\"].value_counts())\n",
    "\n",
    "print(f\"\\nAfter adding this new column called 'popularity', the shape of the data is: \")\n",
    "print(df_cleaned.shape)\n",
    "\n",
    "print(f\"\\nAn article is considered low popularity if it less than {low_threshold} shares\")\n",
    "print(f\"An article is considered medium popularity if it has {low_threshold} - {high_threshold} shares\")\n",
    "print(f\"An article is considered high popularity if it has more than {high_threshold} shares\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing data randomly. We will use 30,000 samples for our training data and the remaining 9,644 samples for our testing data. This part is not necessary yet, so comment it out for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (30000, 55)\n",
      "Testing set shape: (9644, 55)\n"
     ]
    }
   ],
   "source": [
    "# # Shuffle  dataset\n",
    "# df_shuffled = df_cleaned.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Split into training and testing\n",
    "# train_df = df_shuffled.iloc[:30000]\n",
    "# test_df = df_shuffled.iloc[30000:39644]\n",
    "\n",
    "# print(f\"Training set shape: {train_df.shape}\")\n",
    "# print(f\"Testing set shape: {test_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
